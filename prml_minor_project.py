# -*- coding: utf-8 -*-
"""PRML Minor Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sPXpI3xtOUF_PCAfv1lTbETNQXHE1TOF

###Installing the necessary library packages for cleaning comments and preprocessing
"""

!pip install clean-text
!pip install contractions
!pip install nltk
!pip install gensim

"""###Importing the necessary libraries"""

import re
import sys
import random
import numpy as np
import pandas as pd
import contractions
from cleantext import clean
import gensim.downloader as api


from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

import seaborn as sns
import matplotlib.pyplot as plt

"""###Mounting the google drive for file access"""

from google.colab import drive
drive.mount('/content/gdrive')

#loading the database
df=pd.read_csv("/content/gdrive/MyDrive/PRML_Minor/YT.csv", engine='python', encoding='utf-8',error_bad_lines=False)
df.head()

"""##Starting the preprocessing

###Checking null value counts and data type
"""

df.info()

"""###Dropping the null values"""

df.dropna(inplace=True)

df.isnull().sum()

"""###Counting unique users and other details about the dataset"""

df.describe()

"""###Dropping unncessary columns"""

df=df.drop(['User','Video Title','Video Description','Comment (Displayed)','Comment Author'],axis=1)
df

"""###Removing emojis, unnecessary symbols and lowering text"""

df['Comment (Actual)'] = df['Comment (Actual)'].str.replace(r'[^\w\s]+', '')

def clean_text(text):
    if text is None:
        return ''
    return text.lower()


df['Comment (Actual)'] = df['Comment (Actual)'].apply(clean_text)

df

"""###Word embedding"""

# load the pre-trained GloVe word embedding model
glove_model = api.load("glove-wiki-gigaword-100")

# define a function to generate the word embeddings for a text string
def get_word_embeddings(text):
    words = text.split()
    embeddings = []
    for word in words:
        # if the word is in the GloVe model's vocabulary, get its embedding vector
        if word in glove_model.key_to_index:
            embeddings.append(glove_model.get_vector(word))

    # return the mean of the word embeddings as the text's embedding vector
    if embeddings:
        return np.mean(embeddings, axis=0)
    else:
        return np.zeros(glove_model.vector_size)

# apply the function to the text column to generate the word embeddings
df['embedding'] = df['Comment (Actual)'].apply(get_word_embeddings)
df

"""###Applying KMeans Clustering on embedding column"""

embedded_data = np.stack(df['embedding'].values)
kmeans_model = KMeans(n_clusters=50, random_state=42)

kmeans_model.fit(embedded_data)
df['cluster'] = kmeans_model.labels_

# print the size of each cluster
print(df['cluster'].value_counts())

"""###Printing 20 samples from each cluster for manual labelling"""

sampled_df1 = pd.DataFrame()

for i in range(50):
    cluster_data = df[df['cluster'] == i]
    sampled_data = cluster_data.sample(n=20, random_state=5,replace=True)
    sampled_df1 = pd.concat([sampled_df1, sampled_data])

sampled_df1

"""###Manual Labelling of the clusters"""

# create a dictionary of manual labels for each cluster
sampled_df = df
manual_labels = {}

for i in range(50):
    manual_labels[i] = 0
for j in [4, 8, 10, 21, 25, 29, 30, 37]:
  manual_labels[j] = 1

sampled_df['manual_label'] = sampled_df['cluster'].apply(lambda x: manual_labels[x])

print("The number of non-spams vs spams are \n",sampled_df['manual_label'].value_counts())

sampled_df

"""###Creating new database for supervised learning"""

#non-spam
non_spam_df = sampled_df[sampled_df['manual_label'] == 0]
non_spam_sample = non_spam_df.sample(n=2000, random_state=42, replace=True)

#spam
spam_df = sampled_df[sampled_df['manual_label'] == 1]
spam_sample = spam_df.sample(n=500, random_state=42, replace=True)

# concatenate
data_for_supervised = pd.concat([non_spam_sample, spam_sample], ignore_index=True)

data_for_supervised.sample(frac=1)

"""###Flattening+Normalize to create train and test dataset"""

def flatten_array(arr):
    return np.array(arr).flatten()

temp_data = data_for_supervised['embedding']

x_train = np.stack(temp_data.values)
x_train = pd.DataFrame(x_train.reshape(x_train.shape[0], -1))

scaler = MinMaxScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_train1 = pd.DataFrame(x_train_scaled)

y_train = data_for_supervised['manual_label']

x_test = np.array(sampled_df['embedding'].to_list())
x_test = np.reshape(x_test, (x_test.shape[0], -1))

y_test = np.array(sampled_df['manual_label'])

"""###Running the supervised models on original dataset"""

models = {
    'Logistic Regression': LogisticRegression(),
    'Naive Bayes': GaussianNB(),
    'Decision Tree': DecisionTreeClassifier()
}

# Train models
for model_name, model in models.items():
    model.fit(x_train1, y_train)

Accuracy_list = []
Precision_list = []
Recall_list = []
F1_list = []

# Test models
for model_name, model in models.items():
    print(f'Evaluation metrics for {model_name}:')
    y_pred = model.predict(x_test)
    A = accuracy_score(y_test, y_pred)
    B = precision_score(y_test, y_pred)
    C = recall_score(y_test, y_pred)
    D = f1_score(y_test, y_pred)
    Accuracy_list.append(A)
    Precision_list.append(B)
    Recall_list.append(C)
    F1_list.append(D)
    print('Accuracy:',A )
    print('Precision:', B)
    print('Recall:', C)
    print('F1 Score:',D ,"\n")

Model1 = ['Logistic','Gaussian','DecisionTree']


fig, axs = plt.subplots(1, 4, figsize=(14, 4))

sns.barplot(x=Model1, y=Accuracy_list, ax=axs[0])
axs[0].set_title('Accuracy')

sns.barplot(x=Model1, y=Precision_list, ax=axs[1])
axs[1].set_title('Precision')

sns.barplot(x=Model1, y=Recall_list, ax=axs[2])
axs[2].set_title('Recall')

sns.barplot(x=Model1, y=F1_list, ax=axs[3])
axs[3].set_title('F1 Score')

plt.tight_layout()
plt.show()

"""###Applying KNN to test our clustering on labelled dataset"""

X_train, X_test, y_train, y_test = train_test_split(sampled_df['embedding'].tolist(), sampled_df['manual_label'], test_size=0.3, random_state=42)

# Convert list of arrays to 2D numpy array
X_train = np.vstack(X_train)
X_test = np.vstack(X_test)

knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)

y_pred = knn_model.predict(X_test)
print('Accuracy:', accuracy_score(y_test, y_pred))
print('Precision:', precision_score(y_test, y_pred))
print('Recall:', recall_score(y_test, y_pred))
print('F1 Score:', f1_score(y_test, y_pred))

"""###Plotting bar plots for KNN"""

fig, axs = plt.subplots(1, 4, figsize=(7, 5))

sns.barplot(x=['KNN'], y=[accuracy_score(y_test, y_pred)], ax=axs[0])
axs[0].set_title('Accuracy')
sns.barplot(x=['KNN'], y=[precision_score(y_test, y_pred)], ax=axs[1])
axs[1].set_title('Precision')
sns.barplot(x=['KNN'], y=[recall_score(y_test, y_pred)], ax=axs[2])
axs[2].set_title('Recall')
sns.barplot(x=['KNN'], y=[f1_score(y_test, y_pred)], ax=axs[3])
axs[3].set_title('F1 Score')
plt.tight_layout()
plt.show()

"""###Making dataset for applying DBSCAN"""

train_data1, test_data1 = train_test_split(sampled_df, test_size=0.2, random_state=42)
X_train1 = np.array(train_data1['embedding'].tolist())

dbscan = DBSCAN(eps=0.8, min_samples=5)
dbscan.fit(X_train1)

labels = dbscan.labels_

n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
if n_clusters_ > 1:
    silhouette_avg = silhouette_score(X_train1, labels)
    print("For n_clusters =", n_clusters_,
          "The average silhouette_score is :", silhouette_avg)
else:
    print('Only 1 cluster formed, cannot calculate silhouette score.')


train_data1['cluster'] = labels
spam_cluster = train_data1[train_data1['spam'] == 1]['cluster'].mode()[0]
print('Cluster with spam comments:', spam_cluster)

dbscan1 = DBSCAN(eps=0.9, min_samples=5)
dbscan1.fit(X_train1)

labels1 = dbscan1.labels_

n_clusters1 = len(set(labels1)) - (1 if -1 in labels1 else 0)
print("Number of cluster = ",n_clusters1)
if n_clusters1 > 1:
    silhouette_avg1 = silhouette_score(X_train1, labels1)
    print("For n_clusters =", n_clusters1,
          "The average silhouette_score is :", silhouette_avg1)
else:
    print('Only 1 cluster formed, cannot calculate silhouette score.')


train_data1['cluster'] = labels1
spam_cluster1 = train_data1[train_data1['spam'] == 1]['cluster'].mode()[0]
print('Cluster with spam comments:', spam_cluster1)